{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance optimization and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will:\n",
    "\n",
    "* learn how to optimize the performance of an `Operator`,\n",
    "* investigate the effects of optimization in two real-life seismic inversion `Operator`s,\n",
    "* analyze and interpret the performance report displayed after a run.\n",
    "\n",
    "We will rely on preset models and `Operator`s from a seismic inversion problem based on an **isotropic acoustic wave equation**. To run one such `Operator`, in particular a forward modeling operator, we will exploit the `benchmark.py` module. This provides a number of options to configure the simulation and to try out different optimizations. The `benchmark.py` module is intended to let newcomers play with Devito -- and its performance optimizations! -- without having to know anything about its symbolic language, mechanisms and functioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'examples.seismic.benchmark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bec1752caae5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseismic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbenchmark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseismic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'examples.seismic.benchmark'"
     ]
    }
   ],
   "source": [
    "import examples.seismic.benchmark\n",
    "benchmark = examples.seismic.benchmark.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For a full list of options\n",
    "%run $benchmark --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we want Devito to run this `Operator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run $benchmark run -P acoustic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was simple. Of course, we may want to run the same simulation on a bigger grid, with different grid point spacing or space order, and so on. And yes, we'll do this later on in the tutorial. But before scaling up in problem size, we shall take a look at what sort of performance optimizations we'll be able to apply to speed it up.\n",
    "\n",
    "In essence, there are four knobs we can play with to maximize the `Operator` performance (or to see how the performace varies when adding or removing specific transformations):\n",
    "\n",
    "* parallelism,\n",
    "* the Devito Symbolic Engine (DSE) optimization level,\n",
    "* the Devito Loop Engine (DLE) optimization level,\n",
    "* loop blocking auto-tuning.\n",
    "\n",
    "## Shared-memory parallelism\n",
    "\n",
    "We start with the most obvious -- parallelism. Devito implements shared-memory parallelism via OpenMP. To enable it, we would usually simply set the environment variable `DEVITO_OPENMP=1`. However, since we are running in a Jupyter notebook, we can change the relevant configuration option directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from devito import configuration\n",
    "configuration['openmp'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple threads will now be used when running an `Operator`. But how many? And how efficiently? We may be running on a multi-socket node -- how should we treat it, as a \"flatten system\" or what?\n",
    "\n",
    "Devito aims to use distributed-memory parallelism over multi-socket nodes; that is, it allocates one MPI process per socket, and each MPI process should spawn as many OpenMP threads as the number of cores on the socket. Users don't get all this for free, however; a minimal configuration effort is required. But don't worry: as you shall see, it's much simpler than it sounds!\n",
    "\n",
    "For this tutorial, we forget about MPI; we rather focus on enabling OpenMP on a single socket. So, first of all, we want to restrain execution to a single socket -- we want threads to stay on that socket without ever migrating to other cores of the system due to OS scheduling. Are we really on a multi-socket node? And how many _physical_ cores does a socket have? Let's find out. We shall use a very standard tool such as `lscpu` on Linux systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A line beginning with `'NUMA node...'` represents one specific socket. Its value (on the right hand side, after the ':') indicates the ID ranges of its logical cores. For example, if our node consisted of two sockets, each socket having 16 physical cores and 2 hyperthreads per core, we would see something like\n",
    "\n",
    "```\n",
    "...\n",
    "NUMA node0 CPU(s):     0-15,32-47\n",
    "NUMA node1 CPU(s):     16-31,48-63\n",
    "...\n",
    "```\n",
    "\n",
    "Now, say we choose to run on 16 cores of socket 0 (``node0``). We first have to set the following OpenMP environment variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%env OMP_NUM_THREADS=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, 16 threads will be spawned each time an `Operator` is run. They will be killed as soon as the `Operator` has done its job. \n",
    "\n",
    "We also want to **bind** them to the physical cores of socket 0; that is, we want to prevent OS-induced migration. This is known as *thread pinning*. One may use a program such as ``numactl`` or, alternatively, exploit other OpenMP environment variables. If the Intel compiler is at our disposal, we can enforce pinning through the following two-step procedure:\n",
    "\n",
    "* We tell Devito to use the Intel compiler through the special `DEVITO_ARCH` environment variable;\n",
    "* We set the Intel-specific `KMP_HW_SUBSET` and `KMP_AFFINITY` environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Thread pinning\n",
    "%env KMP_HW_SUBSET=16c,1t\n",
    "%env KMP_AFFINITY=compact\n",
    "# Tell Devito to use the Intel compiler\n",
    "%env DEVITO_ARCH=intel\n",
    "# Or, analogously, using the configuration dictionary\n",
    "configuration['compiler'] = 'intel'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one didn't have access to the Intel compiler, it would still be possible to enable thread pinning through analogous mechanisms provided by OpenMP 4.5, namely the `OMP_PLACES` and `OMP_PROC_BIND` variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment if necessary; note that the available GCC version must support OpenMP 4.5 for the following to have effect\n",
    "# %env OMP_PROC_BIND=close\n",
    "# %env OMP_PLACES=cores\n",
    "# %env DEVITO_ARCH=gcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how threading and pinning impact the `Operator` performance.\n",
    "\n",
    "We run the isotropic acoustic forward operator again, but this time with a much larger grid, a 256x256x256 cube, and a more realistic space order, 8. We also shorten the duration by deliberately choosing a very small simulation end time (50 ms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print (\"Run %d\" % i)\n",
    "    %run $benchmark run -P acoustic -so 8 -d 256 256 256 --tn 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: the execution times are stable. This is a symptom that thread pinning is working. In practice, don't forget to check by taking a look at OpenMP reports or using profilers (e.g., Intel VTune) or through user-friendly tools such as `htop`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSE - Devito Symbolic Engine\n",
    "\n",
    "We know how to switch on parallelism. So, it's finally time to see what kind of optimizations can Devito perform. By default, Devito aggressively optimizes all `Operator`s. When running through `benchmark.py`, however, optimizations are left disabled until users explicitly request them. This, hopefully, simplifies initial experimentation and investigation.\n",
    "\n",
    "Let's then dive into to the Devito Symbolic Engine (or DSE) section of this tutorial. It is worth observing that the DSE is one of the distinguishing features of Devito w.r.t. many other stencil frameworks! Why is that? This is what the documentation says about the DSE:\n",
    "\n",
    "> [The DSE performs] Flop-count optimizations - They aim to reduce the operation count of an Operator. These include, for example, code motion, factorization, and detection of cross-stencil redundancies. The flop-count optimizations are performed by routines built on top of SymPy.\n",
    "\n",
    "So the DSE reduces the flop count of `Operator`s. This is particularly useful in the case of complicated PDEs, for example those making extensive use of differential operators. And even more important in high order methods. In such cases, it's not unusual to end up with kernels requiring hundreds of arithmetic operations per grid point calculation. Since Devito doesn't make assumptions about the PDEs, the presence of an optimization system such as the DSE becomes of fundamental importance. In fact, we know that its impact has been remarkable in real-life siesmic inversion operators that have been written on top of Devito (e.g., TTI operators).\n",
    "\n",
    "Let's see what happens enabling the DSE in our acoustic operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Increase Devito logger verbosity to display useful information about the DSE\n",
    "configuration['log_level'] = 'DEBUG'\n",
    "\n",
    "%run $benchmark run -P acoustic -so 8 -d 256 256 256 --tn 50 -dse advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the previous runs, do you note any change ...\n",
    "\n",
    "* in the Devito output reports?\n",
    "* in execution times? why?\n",
    "\n",
    "And why, from a performance analysis point of view, is the DSE useful even though no changes in execution times are observed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DLE - Devito Loop Engine\n",
    "\n",
    "We know how to switch on parallelism and how to leverage the DSE to reduce the flop count of our stencil equations. What's still missing are SIMD vectorization and optimizations for data locality. We won't be able to reach a significant fraction of the attainable machine peak without aggressive loop transformations. Clearly, Devito users don't \"see\" loops -- in fact, they only write maths in Python! -- but the generated code is nothing more than classic C with loop nests for grid function updates. So how do these \"low-level loops\" get optimized? Quoting from the documentation:\n",
    "\n",
    "> Loop optimizations - Examples include SIMD vectorization and loop blocking. These are performed by the Devito Loop Engine (DLE), a sub-module consisting of a sequence of compiler passes manipulating abstract syntax trees [...]\n",
    "\n",
    "In other words, the Devito compiler, through the DLE module, automatically applies loop transformations. The **how it does that**(i.e., manipulation of an intermediate representation), here, is irrelevant; we rather want to understand **what the DLE can do**, **how to use** it and what kind of **tuning** is required to maximize the performance of an `Operator`. As we shall see, using and tuning the DLE will be as simple as switching on some special environment variables!\n",
    "\n",
    "So here's a (non complete) list of transformations that the DLE will automatically apply for you:\n",
    "\n",
    "* SIMD Vectorization\n",
    "* Loop blocking\n",
    "* Optimization of so called \"remainder\" loops\n",
    "* Creation of elemental functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's run the same problem we ran above, but this time with the DLE at maximum level (it's gonna apply **all** optimizations listed above). Can you guess whether the performance will be substantially better? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run $benchmark run -P acoustic -so 8 -d 256 256 256 --tn 50 -dse advanced -dle advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we make the `Operator` run quicker? Yes !\n",
    "\n",
    "What we are missing so far is performance tuning. Take loop blocking, for example. This is a *parametric loop transformation*: its impact will vary depending on block shape, size and scheduling. In the literature, over the years, dozens of different loop blocking strategies have been studied! Even though we used the simplest loop blocking scheme on Earth, we would need **at least** to come up with a block size fitting in some level of cache. Obviously, this is such a low level detail... and we don't want users to waste any time on thinking about these matters.\n",
    "\n",
    "Like other frameworks, Devito can automatically detect a \"sufficiently good\" block size through an *auto-tuning engine*. Let's try it out; in particular, we tell Devito to be \"aggressive\" in the search for block sizes. Being \"aggressive\" means that more time will be spent on auto-tuning, but there's a greater chance of retrieving a better candidate. We can either do this by setting an environment variable (`export DEVITO_AUTOTUNING=aggressive`) or directly through the configuration dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "configuration['autotuning'] = 'aggressive'\n",
    "\n",
    "%run $benchmark run -P acoustic -so 8 -d 256 256 256 --tn 50 -dse advanced -dle advanced -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the addition of `-a` to the arguments list of our benchmarking script. This enables auto-tuning; the `aggressive` setting drives the auto-tuner search.\n",
    "\n",
    "Do you note any difference in performance? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: performance analysis of a TTI forward operator\n",
    "\n",
    "There's another operator that you can try running with our benchmarking script, namely a Tilted Transverse Isotropic operator for forward modeling. This one is even much more interesting than the isotropic acoustic one, as it's representative of a class of real-life wave modeling operators. The physics, and thus the equations, are more complicated, which results in computationally expensive numerical kernels. You can appreciate that yourself by skimming through the genereted code, whose location is displayed after JIT compilation.\n",
    "\n",
    "Here's how to run TTI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run $benchmark run -P tti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise asks you to repeat the same study we did for acoustic, playing with different DSE and DLE levels. Oh, and don't forget, there's another DSE level that you can try besides `advanced`, namely `aggressive`; just give it a go. How does the performance impact of DSE/DLE vary w.r.t. isotropic acoustic operator? And why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sup>This notebook is part of the tutorial \"Optimised Symbolic Finite Difference Computation with Devito\" presented at the University of Sao Paulo April 2019.</sup>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
